---
title: 'Bachelor thesis'
date: 2024-06-12
permalink: /posts/2024/05/bachelor-thesis/
tags:
  - Nonlinear state estimation
  - Chen-Fliess Series
  - Gradient Decent
  - Optimization
---

This is blog for my Bsc thesis about Neural network based state estimation using Lie derivative.


## Introduction

This blog post presents my Bachelor thesis, which addresses the state estimation problem for **nonlinear dynamic systems** using neural networks and Lie derivatives. Traditional model-based observers often struggle due to the need to solve complex partial differential equations (PDEs). This is the work that leverages **gradient descent (GD)** for updating Lie derivatives and utilizes the Chen-Fliess series to model the observer dynamics, resulting in minimal estimation error. To demonstrate the effectiveness of the approach, I apply it to an oscillating system.

## Problem Formulation
Consider a nonlinear system described by the following equations:

$$
\dot{x(t)} = F(x(t)),
y(t) = H(x(t))
$$

The observer is designed based on the linear form:


$$
\dot{z} = Az + By,
\hat{x} = T^{-1}(z)
$$

It is required that 1) (A, B) should be controllable 2) A should be Hurwitz and 3) T is can be inversed!
In additionally, it's required T to be obtain:

$$
\frac{\partial T}{\partial x}(x) F(x) = AT(x) + BH(x)
$$

Now the challenging part is due the part to solve above equation. The cost of solving Lie derivative of T along F(x) is high. So in this Thesis we define the observer we designed as **[Chen-Flies Series](https://github.com/iperezav/CFSpy)**:


$$
\dot{z} = g_0(z) + g_1(z)y + g_2(z)y + ... ,
\hat{x} = h(z)
$$

The Lie derivative of $$h$$ with respect to $$g$$ is given by:
 
$$
Lie_g(h) = \frac{\partial h}{\partial z} g(z)
$$

and for estimation:

$$
\hat{x}_j(t + \Delta t) = \sum_{k=0}^{\infty} \sum_{\mu \in \ell_k^m} L_{\mu} h_j(z(t)) \cdot E_{\mu}(t, t + \Delta t)
$$


To estimate the parameters online, I applied an online least-squares method for the Chen-Fliess series. The cost function is minimized using the following optimization problem:

$$
\min_{\theta_j} J(\theta_j, t) := \frac{1}{2} \int_0^{\Delta} \left( \theta_j^T \phi(t, \delta) - x_j(t + \delta) \right)^2 d\delta.
$$


and

$$
\Phi(t) := \int_0^{\Delta} \phi(t, \delta) \phi^T(t, \delta) d\delta \geq \alpha \Delta I.
$$


This condition ensures persistent excitation, which is necessary for accurate parameter estimation, Hence the optimal solution at time t + $$\Delta$$ can be found:

$$
\theta_j^*(t) = \Phi^{-1}(t) \psi_j(t), \quad \text{where} \quad \psi_j(t) = \int_0^{\Delta} x_j(t + \delta) \phi(t, \delta) d\delta, \quad j = 1, \dots, n.
$$


## Case Study
Let's consider a nonlinear oscillating system described by the following equations:

$$
\dot{x_1} = 1 + x_1^2 x_2 - 4x_1, \quad \dot{x_2} = 3x_1 - x_1^2 x_2
$$


## Results

![image](https://github.com/user-attachments/assets/e7196ac5-2d5e-4d16-9b08-349787b7e98b)

The proposed method for estimation yielded an average error of 0.36, which demonstrates a reasonable level of accuracy considering the complexity of the problem. While not perfect, this error margin falls within an acceptable range for many real-world applications, where slight deviations can be tolerated. Also there is potential of improvements for lower the error by changing hyperparameters in observer and also the way of calculating integrals (simpson and ...).
.

