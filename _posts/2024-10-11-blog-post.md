---
title: 'Causal Normalizing Flow'
date: 2024-10-11
permalink: /posts/2024/10/Causal-normalizing-flow/
tags:
  - causality
  - normalizing-flow
  - likelihood
  - causal-discovery
  - neural-causal
---

In this blog post we discuss the idea of using normalizing flow for cause-effect discovery and multivariate causal discovery with results of three different normalizing flows on Moons dataset. 

## Introduction to Causal discovery

Causal inference has an important role in different areas including machine learning, providing new modelings that can answer new prediction tasks. In order to do causal inference, we need to discover causal relationships between variables. <br>

There are two approaches for causal discovery including constraint and score-based. Constraint-based methods use conditional independence tests in the joint distribution and so these methods output CPDAGs or a graph which belongs to a Markov equivalence class. The problem with these methods is that independence tests need large sample sizes to be reliable. The most well known methods are Spirtes-Glymour-Scheines (SGS), Peter-Clark (PC) and Fast Causal Inference (FCI) [[1]](#1).
In contrast score-based approaches test the validity of a candidate graph G according to some scoring function $S$

$argmax_{G}$ $S(D, G)$ 

where $D$ represents the observational samples for variables. The most well known function S is bayesian information criteria (BIC).

In this article we are going to use normalizing flow as a likelihood-based method for causal discovery. First, we discuss the two-variable case where we want to find which variable is cause and which one is effect. After that, we examine methods and research directions in which we can discover causal relationships between variables using normalizing flow and compare it with other neural causal discovery methods.


## Cause Effect Discovery Using normalizing flow
In the problem of causal discovery, the primary objective is to determine the causal direction between two variables, specifically deciding between two causal models: X->Y or Y->X
A commonly employed approach to select between these models is the use of a likelihood ratio as a score function for model comparison [[2]](#2), [[3]](#3). Also Normalizing Flow (NF) is a likelihood based model which can be used as a score function for causal discovery. Following the Causal Autoregressive flow [[4]](#4) we implement a normalizing flow in pytorch to see how NF behaves between two mentioned causal models. After that we will try to investigate if we can extend NF to multivariate-case comparing it to other neural causal models including Gradient-based Neural Dag Learning [[5]](#5) and Learning sparse nonparametric dags [[6]](#6).

## Normalizing flow
Normalizing flow is a density estimation method which learns complex probability distributions by transforming a simple distribution, for example a gaussian distribution, to a complex one.
NF does this by applying a series of transformations f and using change of variables theorem we can compute the exact likelihood of complex distribution, Fig 1. Examples of such transformations are RealNVP [[7]](#7), Nice [[8]](#8), and GLOW [[9]](#9). We will use RealNVP in this tutorial. For a complete introduction to normalizing flow read the great blog post by Weng, Lilian about [Flow-based Deep Generative Models](https://lilianweng.github.io/posts/2018-10-13-flow-models/).

![image](https://github.com/user-attachments/assets/e709f1f5-bc91-4255-a8a2-71848df33d2b) <br>
[[10]](#10) Weng, Lilian, 2018, Flow-based Deep Generative Models. Lilianweng.github.io.

## Causal normalizing flow

Below there are the equations and the plot of the generated data for the dataset:

### Moon dataset:
outer_circ_x = np.cos(np.linspace(0, np.pi, 100))<br>
outer_circ_y = np.sin(np.linspace(0, np.pi, 100))<br>
inner_circ_x = 1 - np.cos(np.linspace(0, np.pi, 100))<br>
inner_circ_y = 1 - np.sin(np.linspace(0, np.pi, 100)) - 0.5<br>

![image](https://github.com/user-attachments/assets/c2c30e32-51b8-4682-b936-7f8a81064185)
<br>
Fig 2, Moon dataset.





## Normalizing Flow models
In real nvp there are multiple transformations in which half of the variables go through the model without changing and the others go through an affine coupling transformation. You can see each transformation below. 

   $$
   x_{1:d} = z_{1:d}
   $$


   $$
   x_{d+1:D} = z_{d+1:D} \odot e^{s(z_{1:d})} + t(z_{1:d})
   $$


<br>


To increase the expressiveness of transformation the order of variables changes after each stack of normalizing flow, Fig 3.

![Fig3](https://github.com/user-attachments/assets/6c28e1f0-289a-4b09-969c-010ad1db3bc6)
<br>
Fig 3, Stack of flows for non-causal dataset (moon dataset): $X-Y$. Order reversed in each stack.

But suppose that we want to learn the distribution of a causal data set $X->Y$. So we might not need to change the order of variables and apply function $F$ on variable $Y$, Fig 4. 

![Fig4](https://github.com/user-attachments/assets/9173ca49-71a1-4bdd-91cc-c205bc10313a)
<br>
Fig 4, Stack of flows for causal dataset: $X->Y$

And if we want to learn the distribution of data in reverse order $Y->X$ we must apply $F$ to $X$, Fig 5.

![Fig5](https://github.com/user-attachments/assets/32746eb9-d260-49fa-848d-8b7e308056e6)
<br>
Fig 5, Stack of flows for causal dataset: $Y->X$

So based on the above two datasets and the three Normalizing flow models, we will train 3 flows on each datasets and compare the results. 

### Moon dataset:
In Fig 7, training and validation loss of 3 normalizing flow plotted.
As you can see the best loss is related to normalizing flow $X-Y$ which reverses order of variables in each stack of flow, thus it learned the distribution of non-causal dataset more accurate.

![image](https://github.com/user-attachments/assets/933d147c-6589-440c-a5f2-8a2b55d90cd5)

<br>

![image](https://github.com/user-attachments/assets/96711822-27e8-4811-8623-0971d79b0bb6)



Full implementation is available [here](https://github.com/mosi2002/Causal-Normalizing-Flow).

## Multivariate Causal discovery
First, we discuss two approaches for causal discovery that use neural networks like normalizing flow and investigate their problems. After that we discuss if we can use normalizing flow two solve those problems.

### Gradient-based Neural Dag Learning 
Gran Dag presented a neural network in which defined a constraint that cuts some connectivity paths between weights of the neural network. This constraint forces causal relationship between variables and non-causal relationship between variables has zero weights in the corresponding neural network.<br>
It first defined $C$ as the connectivity path between inputs and outputs variables.

$$
\mathcal{C} \triangleq |W^{(L+1)}| \dots |W^{(2)}||W^{(1)}| \in \mathbb{R}_{\geq 0}^{m \times d}
$$

And define the dag constraint on $C$ which is proposed in NO-TEARS [[11]](#11) and learn a dag by optimizing likelihood of data and satisfying this constraint. (This constraint converted the combinatorial nature of the causal discovery algorithm to a continuous optimization method.)

### Learning sparse nonparametric dags 

This method defines a neural network for modeling each variable Each neural network $i$ inputs all variables except the $i-th$ variable and outputs the $i-th$ variable. After that to model causal relationship between variables it used the fact that if the $j-th$ column of weights of the first layer of the neural network become zero then the output $i$ is independent of variable $j$. Therefore by satisfying the same constraint like notears it models the causal relationship between all variables.

### Gradient-based Neural Dag Learning vs Learning sparse nonparametric dags.

**Gran Dag** models the causal relationship between variables with just one neural network by satisfying the causal connectivity constraint between weights of the neural network which is definitely hard to find such that constraint. In contrast, **Sparse nonparametric dags** uses n (number of variables) neural networks but cut dependency between variables by just zeroing out weights of the first layer and satisfying the dag constraint by just weights of first layers of all n neural networks. It seems that this constraint is easier to optimize than **Gran-dag** which uses weights of all layers of the neural network. But to reach this easier constraint it has to use n neural networks instead of one.

### Multivariate Causal discovery with normalizing flow?
I have been researching on this question trying to figure out how can we model causal relationship between multiple variables using normalizing flow which does not have the problem of learning n neural networks instead of one in **Sparse nonparametric dags**, and hard to optimize dag constraint in **Gran Dag**.


## References
<a name="1">[1]</a> Peter Spirtes, Clark N Glymour, Richard Scheines, and David Heckerman. Causation, prediction, and search. MIT press, 2000.

<a name="2">[2]</a> Zhang, K. and Hyvärinen, A., 2016. Nonlinear functional causal models for distinguishing cause from effect. Statistics and Causality; John Wiley & Sons, Inc.: New York, NY, USA, pp.185-201.


<a name="3">[3]</a> Monti, R.P., Zhang, K. and Hyvärinen, A., 2020, August. Causal discovery with general non-linear relationships using non-linear ica. In Uncertainty in artificial intelligence (pp. 186-195). PMLR.

<a name="1">[4]</a> Khemakhem, I., Monti, R., Leech, R. and Hyvarinen, A., 2021, March. Causal autoregressive flows. In International conference on artificial intelligence and statistics (pp. 3520-3528). PMLR.

<a name="1">[5]</a> Lachapelle, S., Brouillard, P., Deleu, T. and Lacoste-Julien, S., 2019. Gradient-based neural dag learning. arXiv preprint arXiv:1906.02226.

<a name="1">[6]</a> Zheng, X., Dan, C., Aragam, B., Ravikumar, P. and Xing, E., 2020, June. Learning sparse nonparametric dags. In International Conference on Artificial Intelligence and Statistics (pp. 3414-3425). PMLR.

<a name="1">[7]</a> Dinh, L., Sohl-Dickstein, J. and Bengio, S., 2016. Density estimation using real nvp. arXiv preprint arXiv:1605.08803.

<a name="1">[8]</a> Dinh, L., Krueger, D. and Bengio, Y., 2014. Nice: Non-linear independent components estimation. arXiv preprint arXiv:1410.8516.

<a name="1">[9]</a> Kingma, D.P. and Dhariwal, P., 2018. Glow: Generative flow with invertible 1x1 convolutions. Advances in neural information processing systems, 31.

<a name="1">[10]</a> Weng, Lilian, 2018, Flow-based Deep Generative Models. In lilianweng.github.io, [Link](https://lilianweng.github.io/posts/2018-10-13-flow-models/)

<a name="1">[11]</a> Zheng, X., Aragam, B., Ravikumar, P.K. and Xing, E.P., 2018. Dags with no tears: Continuous optimization for structure learning. Advances in Neural Information Processing Systems, 31.
