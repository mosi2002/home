---
title: 'MSE vs RNN'
date: 2024-05-08
permalink: /posts/2024/05/MSE-vs-RNN/
tags:
  - Synchronization of Communication systems
  - MSE method
  - RNN, LSTM
---

This Blog post contains a comparison study of a Recurrent neural network (RNN) model and a method from this [paper](https://folk.ntnu.no/skoge/prost/proceedings/ifac2008/data/papers/1383.pdf) published by [S. Vahid Naghavi](https://scholar.google.com/citations?user=5bT9h5IAAAAJ&hl=en) for a chaotic system. As a reminder, I used states \( x \) as input so these methods can be compared!

[Code](https://github.com/mosi2002/Rnn-based-estimation-Vs-mse-estimation) for this review.
## Introduction to main Problem
The importance of synchronization of two coupled chaotic systems was appreciated soon and this topic aroused great interest as a potential means for **Communication** (Kolumban et al, 1997; Tse et al, 2003). In recent years, a great deal of effort has been devoted to extending chaotic communication applications to the field of secure communications. A detailed survey of chaotic secure communication systems is presented by Yang (2004). As the chaos synchronization problem can be reformulated as an observer design problem, the observer-based approach becomes one of the most attractive techniques for chaotic systems. This kind of approach has been extensively investigated in recent research works by Grassi and Mascolo (2002); Morgul (1996) and Solak (1997); Nijmeijer and Mareels (1997); Ushio (1999); Celikovsky and Chen (2002). Neural networks (NNs) have been recognized as valuable tools that offer simple solutions to difficult problems in various science and engineering fields due to their inherent adaptability and universal approximation properties (Suykens, 1996; Luo et al, 1997; Cherkassky et al, 1998). Especially in the area of control, neural networks have experienced increased interest in the last decade.

## MSE Problem Formulation
Refer to the [paper](https://folk.ntnu.no/skoge/prost/proceedings/ifac2008/data/papers/1383.pdf):

Let's assume the system:

$$
\begin{cases}
x(k+1) = A \cdot x(k) + f(x(k), y(k), k) \\
y(k) = C \cdot x(k)
\end{cases}
$$
the paper suggest the above non-linear term f(x(k), y(k), k) can be written:
$$
f(x(k), y(k), k) =g_1(y(k), k)Hx(k) + g_2(y(k), k)
$$

then we can introduce:
$$
\rho(k) = g_1(y(k), k) \\
\bar{A} (\rho(k)) = A + \rho(k)H
$$
so the system can be written:
$$
x(k+1)  = \bar{A} (\rho(k))x(k) + g_2(y(k), k) \\
y(k+1) = Cx(k)
$$

then paper suggest propose state estimation methid based on MSE and derive the equations based on **Neural Network**.
For updating L which is suggest for the method is paper:
$$
J = (y(k) - \hat{y(k)})^T(y(k) - \hat{y(k)}) \\
\frac{\partial J}{\partial L}  = -2(y(k) - \hat{y(k)})^TC\frac{\partial \hat{x(k)}}{\partial L}\\
\frac{\partial \hat{x(k)}}{\partial L} = ((A + \rho(k-1)H + \frac{\partial g_2}{\partial y(k-1)}.C ) - L.C)(y(k-2) - \hat{y(k-2)}) + (y(k-1) - \hat{y(k-1)}).ones(n)\\
L(k+1) = L(k) - \zeta(\frac{\partial J}{\partial L})^T
$$
Then after updating L from above algorithm we can have out estimations about states of the system:

$$
\hat{x(k+1)} = (W_s - T).\hat{x(k)} + W_I.y(k) + g_2(y(k), k) \\
Where (W_s - T) = (A+ \rho(k).H - L.C) \\
W_I =L \\  I(k) =y(k)
$$

![Fig 1](/images/nf/2.png)<br>
Fig 1, Comparison of True States and Estimated States

![Fig 2](/images/nf/3.png)<br>
Fig 2, Comparison of True output and Estimated output



## LSTM Problem Formulation


## Discussion

## References
<a name="1">[1]</a> Peter Spirtes, Clark N Glymour, Richard Scheines, and David Heckerman. Causation, prediction, and search. MIT press, 2000.

<a name="2">[2]</a> Zhang, K. and Hyvärinen, A., 2016. Nonlinear functional causal models for distinguishing cause from effect. Statistics and Causality; John Wiley & Sons, Inc.: New York, NY, USA, pp.185-201.

<a name="3">[3]</a> Monti, R.P., Zhang, K. and Hyvärinen, A., 2020, August. Causal discovery with general non-linear relationships using non-linear ICA. In Uncertainty in artificial intelligence (pp. 186-195). PMLR.

<a name="4">[4]</a> Khemakhem, I., Monti, R., Leech, R. and Hyvarinen, A., 2021, March. Causal autoregressive flows. In International conference on artificial intelligence and statistics (pp. 3520-3528). PMLR.

<a name="5">[5]</a> Lachapelle, S., Brouillard, P., Deleu, T. and Lacoste-Julien, S., 2019. Gradient-based neural DAG learning. arXiv preprint arXiv:1906.02226.

<a name="6">[6]</a> Zheng, X., Dan, C., Aragam, B., Ravikumar, P. and Xing, E., 2020, June. Learning sparse nonparametric DAGs. In International Conference on Artificial Intelligence and Statistics (pp. 3414-3425). PMLR.
